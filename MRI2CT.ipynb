{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MRI2CT.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOvPVz6HoHLqPVQFvPvmh64"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"QU-Sde_vNarm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606041418951,"user_tz":-210,"elapsed":25707,"user":{"displayName":"Soheib Takhtardeshir","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GikDsGcdXzPRT0E9ePB2VsORotDCDzVXhRQdsL2jQ=s64","userId":"07721176589740380310"}},"outputId":"0772f6be-0ff0-4426-91d3-87318fa9e94c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0b5c6GpKzn8y"},"source":["# -*- coding: utf-8 -*-\n","\"\"\"CycleGAN.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1UHc0k4iJ8ytd157KfTfu-m4gaB3iZHUd\n","\"\"\"\n","\n","# ! [ ! -z \"$COLAB_GPU\" ] && pip install -q git+https://github.com/tensorflow/examples.git\n","import tensorflow as tf\n","import os\n","import time\n","from numba import jit, cuda\n","from PIL import Image\n","from matplotlib import pyplot as plt\n","from IPython import display\n","import cv2\n","import tensorflow_datasets as tfds\n","# from tensorflow_examples.models.pix2pix import pix2pix\n","\n","import os\n","import time\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","import numpy as np\n","from tqdm import tqdm\n","from tensorflow.python.client import device_lib\n","#print(f\"GPU : {tf.config.list_physical_devices('GPU')}\")\n","#print(tf.__version__)\n","\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","\"\"\"\n","_URL = 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip'\n","path_to_zip = tf.keras.utils.get_file('horse2zebra.zip', origin=_URL, extract=True)\n","\n","print(path_to_zip)\n","PATH = os.path.join(os.path.dirname(path_to_zip), 'horse2zebra/')\n","print(PATH)\n","\"\"\"\n","\n","#from google.colab import drive\n","#drive.mount('/content/drive')\n","\n","PATH = '/content/drive/MyDrive/MRI2CT'\n","\n","BUFFER_SIZE = 1000\n","BATCH_SIZE = 32\n","IMG_WIDTH = 256\n","IMG_HEIGHT = 256\n","IMG_CHANNEL = 3\n","\n","train_CT_file = os.path.join(PATH, \"CT-Full-Dataset\")\n","train_IR_name = os.listdir(train_CT_file)\n","train_MRI_file = os.path.join(PATH, \"cycleGAN_MRI\")\n","train_VL_name = os.listdir(train_MRI_file)\n","\n","train_IR_number = len(train_IR_name)\n","train_IR = np.zeros((train_IR_number, IMG_WIDTH, IMG_HEIGHT, IMG_CHANNEL), dtype=np.float32)\n","train_VL_number = len(train_VL_name)\n","train_VL = np.zeros((train_VL_number, IMG_WIDTH, IMG_HEIGHT, IMG_CHANNEL), dtype=np.float32)\n","\n","\n","print(len(tf.config.experimental.list_physical_devices(\"GPU\")))\n","\n","#len(train_IR_name)\n","\n","for i in tqdm(range(len(train_IR_name))):\n","    img = np.array(Image.open(os.path.join(train_MRI_file, train_IR_name[i])))\n","    img=cv2.resize(img,(256,256))\n","    img = img/127.5-1\n","    train_IR[i,:,:,0] = img\n","    train_IR[i, :, :, 1] = img\n","    train_IR[i, :, :, 2] = img\n","\n","\n","\n","\n","print(\"Training MRI Dataset done ...\")\n","\n","#len(train_VL_name)\n","for i in tqdm(range(len(train_VL_name))):\n","    img = np.array(Image.open(os.path.join(train_CT_file, train_VL_name[i])))\n","    img=cv2.resize(img,(256,256))\n","    img = img/127.5-1\n","    train_VL[i,:,:,0] = img\n","    train_VL[i, :, :, 1] = img\n","    train_VL[i, :, :, 2] = img\n","\n","print(\"Training CT Dataset done ...\")\n","\n","#np.save(\"Train-MRI\", train_IR)\n","#np.save(\"Train-CT\", train_VL)\n","\n","#train_IR = np.load('/home/sm/PycharmProjects/MRI-To-CT/venv/lib/Train-MRI.npy')\n","\n","#train_VL = np.load('/home/sm/PycharmProjects/MRI-To-CT/venv/lib/Train-CT.npy')\n","\n","plt.figure()\n","plt.imshow((train_VL[0]) / 2 + 0.5)\n","plt.figure()\n","plt.imshow(train_IR[0]/2 + 0.5)\n","plt.show()\n","\n","\n","\n","\n","class InstanceNormalization(tf.keras.layers.Layer):\n","    \"\"\"Instance Normalization Layer (https://arxiv.org/abs/1607.08022).\"\"\"\n","\n","    def __init__(self, epsilon=1e-5):\n","        super(InstanceNormalization, self).__init__()\n","        self.epsilon = epsilon\n","\n","    def get_config(self):\n","        config = super(InstanceNormalization, self).get_config().copy()\n","        config.update({\"epsilon\": self.epsilon})\n","        return config\n","\n","    def build(self, input_shape):\n","        self.scale = self.add_weight(\n","            name='scale',\n","            shape=input_shape[-1:],\n","            initializer=tf.random_normal_initializer(1., 0.02),\n","            trainable=True)\n","\n","        self.offset = self.add_weight(\n","            name='offset',\n","            shape=input_shape[-1:],\n","            initializer='zeros',\n","            trainable=True)\n","\n","    def call(self, x):\n","        mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n","        inv = tf.math.rsqrt(variance + self.epsilon)\n","        normalized = (x - mean) * inv\n","        return self.scale * normalized + self.offset\n","\n","\n","def downsample(filters, size, norm_type='batchnorm', apply_norm=True):\n","    \"\"\"Downsamples an input.\n","    Conv2D => Batchnorm => LeakyRelu\n","    Args:\n","      filters: number of filters\n","      size: filter size\n","      norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n","      apply_norm: If True, adds the batchnorm layer\n","    Returns:\n","      Downsample Sequential Model\n","    \"\"\"\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","\n","    result = tf.keras.Sequential()\n","    result.add(\n","        tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n","                               kernel_initializer=initializer, use_bias=False))\n","\n","    if apply_norm:\n","        if norm_type.lower() == 'batchnorm':\n","            result.add(tf.keras.layers.BatchNormalization())\n","        elif norm_type.lower() == 'instancenorm':\n","            result.add(InstanceNormalization())\n","\n","    result.add(tf.keras.layers.LeakyReLU())\n","\n","    return result\n","\n","\n","def upsample(filters, size, norm_type='batchnorm', apply_dropout=False):\n","    \"\"\"Upsamples an input.\n","    Conv2DTranspose => Batchnorm => Dropout => Relu\n","    Args:\n","      filters: number of filters\n","      size: filter size\n","      norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n","      apply_dropout: If True, adds the dropout layer\n","    Returns:\n","      Upsample Sequential Model\n","    \"\"\"\n","\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","\n","    result = tf.keras.Sequential()\n","    result.add(\n","        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n","                                        padding='same',\n","                                        kernel_initializer=initializer,\n","                                        use_bias=False))\n","\n","    if norm_type.lower() == 'batchnorm':\n","        result.add(tf.keras.layers.BatchNormalization())\n","    elif norm_type.lower() == 'instancenorm':\n","        result.add(InstanceNormalization())\n","\n","    if apply_dropout:\n","        result.add(tf.keras.layers.Dropout(0.5))\n","\n","    result.add(tf.keras.layers.ReLU())\n","\n","    return result\n","\n","\n","def generator(output_channels, norm_type='batchnorm'):\n","    \"\"\"Modified u-net generator model (https://arxiv.org/abs/1611.07004).\n","    Args:\n","      output_channels: Output channels\n","      norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n","    Returns:\n","      Generator model\n","    \"\"\"\n","\n","    down_stack = [\n","        downsample(64, 4, norm_type, apply_norm=False),  # (bs, 128, 128, 64)\n","        downsample(128, 4, norm_type),  # (bs, 64, 64, 128)\n","        downsample(256, 4, norm_type),  # (bs, 32, 32, 256)\n","        downsample(512, 4, norm_type),  # (bs, 16, 16, 512)\n","        downsample(512, 4, norm_type),  # (bs, 8, 8, 512)\n","        downsample(512, 4, norm_type),  # (bs, 4, 4, 512)\n","        downsample(512, 4, norm_type),  # (bs, 2, 2, 512)\n","        downsample(512, 4, norm_type),  # (bs, 1, 1, 512)\n","    ]\n","\n","    up_stack = [\n","        upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 2, 2, 1024)\n","        upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 4, 4, 1024)\n","        upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 8, 8, 1024)\n","        upsample(512, 4, norm_type),  # (bs, 16, 16, 1024)\n","        upsample(256, 4, norm_type),  # (bs, 32, 32, 512)\n","        upsample(128, 4, norm_type),  # (bs, 64, 64, 256)\n","        upsample(64, 4, norm_type),  # (bs, 128, 128, 128)\n","    ]\n","\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","    last = tf.keras.layers.Conv2DTranspose(\n","        output_channels, 4, strides=2,\n","        padding='same', kernel_initializer=initializer,\n","        activation='tanh')  # (bs, 256, 256, 3)\n","\n","    concat = tf.keras.layers.Concatenate()\n","\n","    inputs = tf.keras.layers.Input(shape=[None, None, 3])\n","    x = inputs\n","\n","    # Downsampling through the model\n","    skips = []\n","    for down in down_stack:\n","        x = down(x)\n","        skips.append(x)\n","\n","    skips = reversed(skips[:-1])\n","\n","    # Upsampling and establishing the skip connections\n","    for up, skip in zip(up_stack, skips):\n","        x = up(x)\n","        x = concat([x, skip])\n","\n","    x = last(x)\n","\n","    return tf.keras.Model(inputs=inputs, outputs=x)\n","\n","\n","Generator = generator(IMG_CHANNEL, norm_type='instancenorm')\n","tf.keras.utils.plot_model(Generator, show_shapes=True, dpi=64)\n","\n","\n","def discriminator(norm_type='batchnorm', target=True):\n","    \"\"\"PatchGan discriminator model (https://arxiv.org/abs/1611.07004).\n","    Args:\n","      norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n","      target: Bool, indicating whether target image is an input or not.\n","    Returns:\n","      Discriminator model\n","    \"\"\"\n","\n","    print(norm_type.lower())\n","    initializer = tf.random_normal_initializer(0., 0.02)\n","\n","    inp = tf.keras.layers.Input(shape=[None, None, 3], name='input_image')\n","    x = inp\n","\n","    if target:\n","        tar = tf.keras.layers.Input(shape=[None, None, 3], name='target_image')\n","        x = tf.keras.layers.concatenate([inp, tar])  # (bs, 256, 256, channels*2)\n","\n","    down1 = downsample(64, 4, norm_type, False)(x)  # (bs, 128, 128, 64)\n","    down2 = downsample(128, 4, norm_type)(down1)  # (bs, 64, 64, 128)\n","    down3 = downsample(256, 4, norm_type)(down2)  # (bs, 32, 32, 256)\n","\n","    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n","    conv = tf.keras.layers.Conv2D(\n","        512, 4, strides=1, kernel_initializer=initializer,\n","        use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n","\n","    if norm_type.lower() == 'batchnorm':\n","        norm1 = tf.keras.layers.BatchNormalization()(conv)\n","    elif norm_type.lower() == 'instancenorm':\n","        norm1 = InstanceNormalization()(conv)\n","\n","    leaky_relu = tf.keras.layers.LeakyReLU()(norm1)\n","\n","    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n","\n","    last = tf.keras.layers.Conv2D(\n","        1, 4, strides=1,\n","        kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n","\n","    if target:\n","        return tf.keras.Model(inputs=[inp, tar], outputs=last)\n","    else:\n","        return tf.keras.Model(inputs=inp, outputs=last)\n","\n","\n","discriminator_diagram = discriminator(norm_type='instancenorm', target=False)\n","tf.keras.utils.plot_model(discriminator_diagram, show_shapes=True, dpi=64)\n","\n","VL_dataset = tf.data.Dataset.from_tensor_slices(train_VL).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","IR_dataset = tf.data.Dataset.from_tensor_slices(train_IR).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","\n","sample_VL = next(iter(VL_dataset))\n","sample_IR = next(iter(IR_dataset))\n","\n","generator_g = generator(IMG_CHANNEL, norm_type='instancenorm')\n","generator_f = generator(IMG_CHANNEL, norm_type='instancenorm')\n","\n","discriminator_x = discriminator(norm_type='instancenorm', target=False)\n","discriminator_y = discriminator(norm_type='instancenorm', target=False)\n","\n","print(train_VL.shape)\n","print(train_IR.shape)\n","print(sample_VL.shape)\n","print(sample_IR.shape)\n","\n","to_IR = generator_g(sample_VL)\n","to_VL = generator_f(sample_IR)\n","plt.figure(figsize=(8, 8))\n","contrast = 8\n","\n","imgs = [train_VL, to_IR, train_IR, to_VL]\n","title = ['MRI', 'To CT', 'CT', 'To MRI']\n","\n","for i in range(len(imgs)):\n","    plt.subplot(2, 2, i + 1)\n","    plt.title(title[i])\n","    if i % 2 == 0:\n","        plt.imshow(imgs[i][0] * 0.5 + 0.5)\n","    else:\n","        plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\n","plt.show()\n","\n","dis = discriminator_x(to_IR)\n","# print(dis)\n","print(dis.shape)\n","\n","LAMBDA = 10\n","\n","loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","\n","def discriminator_loss(real, generated):\n","    real_loss = loss_obj(tf.ones_like(real), real)\n","\n","    generated_loss = loss_obj(tf.zeros_like(generated), generated)\n","\n","    total_disc_loss = real_loss + generated_loss\n","\n","    return total_disc_loss * 0.5\n","\n","\n","def generator_loss(generated):\n","    return loss_obj(tf.ones_like(generated), generated)\n","\n","\n","def calc_cycle_loss(real_image, cycled_image):\n","    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n","\n","    return LAMBDA * loss1\n","\n","\n","def identity_loss(real_image, same_image):\n","    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n","    return LAMBDA * 0.5 * loss\n","\n","\n","generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n","\n","checkpoint_path = \"/home/sm/MRI2CT/Dataset/checkpoint\"\n","\n","ckpt = tf.train.Checkpoint(generator_g=generator_g,\n","                           generator_f=generator_f,\n","                           discriminator_x=discriminator_x,\n","                           discriminator_y=discriminator_y,\n","                           generator_g_optimizer=generator_g_optimizer,\n","                           generator_f_optimizer=generator_f_optimizer,\n","                           discriminator_x_optimizer=discriminator_x_optimizer,\n","                           discriminator_y_optimizer=discriminator_y_optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","# if a checkpoint exists, restore the latest checkpoint.\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print('Latest checkpoint restored!!')\n","\n","EPOCHS = 1\n","\n","\n","def generate_images(model, test_input):\n","    prediction = model(test_input)\n","\n","    plt.figure(figsize=(12, 12))\n","\n","    display_list = [test_input[0], prediction[0]]\n","    title = ['Input Image', 'Predicted Image']\n","\n","    for i in range(2):\n","        plt.subplot(1, 2, i + 1)\n","        plt.title(title[i])\n","        # getting the pixel values between [0, 1] to plot it.\n","        plt.imshow(display_list[i] * 0.5 + 0.5)\n","        plt.axis('off')\n","    plt.show()\n","\n","\n","generate_images(generator_g, sample_VL)\n","\n","\n","@tf.function\n","def train_step(real_x, real_y):\n","    # persistent is set to True because the tape is used more than\n","    # once to calculate the gradients.\n","    with tf.GradientTape(persistent=True) as tape:\n","        # Generator G translates X -> Y\n","        # Generator F translates Y -> X.\n","\n","        fake_y = generator_g(real_x, training=True)\n","        cycled_x = generator_f(fake_y, training=True)\n","\n","        fake_x = generator_f(real_y, training=True)\n","        cycled_y = generator_g(fake_x, training=True)\n","\n","        # same_x and same_y are used for identity loss.\n","        same_x = generator_f(real_x, training=True)\n","        same_y = generator_g(real_y, training=True)\n","\n","        disc_real_x = discriminator_x(real_x, training=True)\n","        disc_real_y = discriminator_y(real_y, training=True)\n","\n","        disc_fake_x = discriminator_x(fake_x, training=True)\n","        disc_fake_y = discriminator_y(fake_y, training=True)\n","\n","        # calculate the loss\n","        gen_g_loss = generator_loss(disc_fake_y)\n","        gen_f_loss = generator_loss(disc_fake_x)\n","\n","        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n","\n","        # Total generator loss = adversarial loss + cycle loss\n","        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n","        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n","\n","        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n","        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n","\n","    # Calculate the gradients for generator and discriminator\n","    generator_g_gradients = tape.gradient(total_gen_g_loss, generator_g.trainable_variables)\n","    generator_f_gradients = tape.gradient(total_gen_f_loss, generator_f.trainable_variables)\n","\n","    discriminator_x_gradients = tape.gradient(disc_x_loss, discriminator_x.trainable_variables)\n","    discriminator_y_gradients = tape.gradient(disc_y_loss, discriminator_y.trainable_variables)\n","\n","    # Apply the gradients to the optimizer\n","    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, generator_g.trainable_variables))\n","\n","    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, generator_f.trainable_variables))\n","\n","    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients, discriminator_x.trainable_variables))\n","\n","    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients, discriminator_y.trainable_variables))\n","\n","\n","for epoch in range(EPOCHS):\n","    print(epoch)\n","    start = time.time()\n","\n","    n = 0\n","    for image_x, image_y in tf.data.Dataset.zip((VL_dataset, IR_dataset)):\n","        train_step(image_x, image_y)\n","        if n % 10 == 0:\n","            print('.', end='')\n","        n += 1\n","\n","    clear_output(wait=True)\n","    # Using a consistent image (sample_horse) so that the progress of the model\n","    # is clearly visible.\n","    generate_images(generator_g, sample_VL)\n","\n","    if (epoch + 1) % EPOCHS == 0:\n","        ckpt_save_path = ckpt_manager.save()\n","        print('Saving checkpoint for epoch {} at {}'.format(epoch + 1, ckpt_save_path))\n","\n","    print('Time taken for epoch {} is {} sec\\n'.format(epoch + 1, time.time() - start))\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_NdD-ZDnxuK7"},"source":["# New Section"]}]}